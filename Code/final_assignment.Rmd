---
title: "final_assignment"
author: "JC De Orellana Sanchez"
date: "2024-04-15"
output:
  pdf_document: default
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Medical Disinformation with Political Intent



```{python, eval=FALSE, python.reticulate = FALSE}
## YouTube API
### Python code to access the YouTube API

import os
import requests
import re # For regular expressions to sanite the playlist name to create a valid filename
import json
import time  '''to call the .sleep() method to include a pause that respects potential respect API limits'''
import pandas as pd
import matplotlib.pyplot as plt

from googleapiclient.discovery import build
from youtube_api import YouTubeDataAPI
from youtube_api import parsers as P
from datetime import datetime

YT_KEY = os.environ.get('YOUTUBE_API_KEY') # Key is saved in ~/.zshrc
api_key = YT_KEY
yt = YouTubeDataAPI(YT_KEY)
                    
#-------------------------------------------------------------------------------
class DateTimeEncoder(json.JSONEncoder):
    """Custom encoder for datetime objects."""
    def default(self, obj):
        if isinstance(obj, datetime):
            return obj.isoformat()  # Convert datetime to ISO format string
        # Let the base class default method raise the TypeError
        return super().default(obj)
```

### Function to get the ID of a YouTube channel using the channel's display_name

```{python, eval=FALSE, python.reticulate = FALSE}
"""Function to get the ID of a YouTube channel using the channel's display_name"""

def find_channel_id_by_display_name(display_name, api_key):
    # Define the endpoint and parameters for the request
    search_url = 'https://www.googleapis.com/youtube/v3/search'
    params = {
        'part': 'snippet',
        'q': display_name,
        'type': 'channel',
        'key': api_key
    }
    
    # Make the GET request
    response = requests.get(search_url, params=params)
    
    # Check if the request was successful
    if response.status_code == 200:
        # Parse the response
        search_results = response.json()
        
        # Loop through the search results and find the matching channel
        for item in search_results.get('items', []):
            # Check if the snippet's channel title closely matches the search query (display name)
            if item['snippet']['channelTitle'].lower() == display_name.lower():
                return item['snippet']['channelId']
        
        # If no exact match is found, return a message
        return "No exact match found for the display name."
    else:
        # If the request was not successful, return an error message with the status code
        return f"Failed to retrieve data: HTTP Status Code {response.status_code}"
```

### Function to get the ID of a YouTube channel using the Channel's username

```{python, eval=FALSE, python.reticulate = FALSE}
''' Function to get the ID of a YouTube channel using the Channel's username'''

def get_channel_id(channel_username, api_key):
    try:
        # Make a GET request to the YouTube Data API's channels.list method
        response = requests.get(
            'https://www.googleapis.com/youtube/v3/channels',
            params={
                'part': 'id',
                'forUsername': channel_username,
                'key': api_key
            }
        )
        # Check if the request was successful
        if response.status_code == 200: # status code of 200 indicates successful request (API endpoint was reached without any errors)
            response_json = response.json() # parse the JSON content returned by the API into a Python dictionary
            items = response_json.get('items', []) # retrieves 'items' list from JSON object
            
            if items:
                # Assuming the first item is the correct channel
                channel_id = items[0]['id']
                return channel_id
            else:
                return "No channel found for the given username."
        else:
            return "Failed to retrieve data: HTTP Status Code {}".format(response.status_code)
    except Exception as e:
        # Handle potential exceptions, such as a network error
        return "An error occurred: {}".format(e)
```

### Objects to use in the `find_channel_id...` and `get_channel_id` functions

```{python, eval= FALSE, python.reticulate = FALSE}
chan_names_list = ['willaxtv', 'PBO'] # List of channel usernames
display_name = 'PBO'  # The display name of the channel I am searching for'''
```

### Search for YouTube channel IDs using the functions and objects defined above, and get the chanel's playlists using the `get_playlists()` function from the `youtube_api`

```{python, eval=FALSE, python.reticulate = FALSE}
# Dictionary to store channel usernames and their IDs
channel_ids = {}

# Loop through each channel username in the list
for channel_username in chan_names_list:
    # Call get_channel_id for each username
    channel_id = get_channel_id(channel_username, api_key)
    if channel_id == "No channel found for the given username.":
        # Using the find_channel_id_by_display_name
        channel_id = find_channel_id_by_display_name(channel_username, api_key) 
    # Store the result in the dictionary
    channel_ids[channel_username] = channel_id

'''Debug: Print resulting dictionary
 print(channel_ids)'''
 
filename = 'covid_disinfo_peru_yt_playlists.json'
# Load existing data if the file exists
if os.path.exists(filename):
    with open(filename, 'r') as file:
        data = json.load(file)
else:
    data = {}

# Loop through each channel ID to get and process playlists
for channel_username, channel_id in channel_ids.items():
    '''print(f"Retrieving playlists for channel: {channel_username} with ID {channel_id}")'''

    # Get the playlists for the channel
    ch_playlists = yt.get_playlists(channel_id, next_page_token=False, parser=P.parse_playlist_metadata, part=['id', 'snippet', 'contentDetails'])
    
    # Compile a regular expression pattern for the keywords, making it case-insensitive
    pattern = re.compile(r'barba|rey|butters|leiva|beto|thorndike|vivo', re.IGNORECASE)

    # List to store matched playlists
    matched_playlists = []

    # Iterate over the playlists and search for the pattern in their names
    for playlist in ch_playlists:
        if pattern.search(playlist['playlist_name']):
            matched_playlists.append(playlist)
    # Output the matched playlists
    print(f"Matched playlists for {channel_username}:")
    
    for match in matched_playlists:
        print(match['playlist_name'])
        #print(match)
    
    # Convert datetime objects in matched_playlists to strings
    for playlist in matched_playlists:
        # Assuming 'collection_date' is the datetime object to convert
        if 'collection_date' in playlist and isinstance(playlist['collection_date'], datetime):
            playlist['collection_date'] = playlist['collection_date'].isoformat()
       
    # Updates data with the new matched_playlists for each channel_username
    data[channel_username] = matched_playlists

# Write the updated data back to the JSON file
with open(filename, 'w') as file:
    json.dump(data, file, indent=4)
```

### Iterate over playlist json file to get details needed to retrieve videos with `get_videos_from_playlist_id()` function and save them in independent files

```{python, eval=FALSE, python.reticulate = FALSE}
'''open jason dictionary
for loop to iterate in the playlists dictionary
get playlist_id and _name
clean playlist_name to use it in file name. Regex and shorting name yt function to get videos (and their metadata) from playlist store the data in a json file
'''

# Open the playlists file
filename = 'covid_disinfo_peru_yt_playlists.json'

# Load existing data if the file exists
if os.path.exists(filename):
    with open(filename, 'r') as file:
        channels_data = json.load(file)
else:
    raise FileNotFoundError("The file: '" + filename + "' doesn't exist. Check its name or path.")

for channel_name, playlists in channels_data.items():
    #print(f"Channel: {channel_name}")
    for playlist in playlists:
        playlist_id = playlist['playlist_id']
        playlist_name = playlist['playlist_name']
        print(f"Processing playlist: {playlist_name} with ID: {playlist_id}")
        
        # Sanitize the playlist name to create a valid filename. Removes characters not allowed in filenames and limits the length
        safe_playlist_name = re.sub(r'[^\w\s-]', '', playlist_name).strip()[:20].replace(' ', '_')
        fetched_vids = f"{safe_playlist_name}_2020-21_videos.json"

         # Call the function to get videos from this playlist ID
        video_metadata = yt.get_videos_from_playlist_id(playlist_id=playlist_id, 
                                                        next_page_token=None, 
                                                        parser=P.parse_video_url, 
                                                        part=['snippet'], 
                                                        max_results=200000)
        # Save the fetched video metadata to a file named after the playlist
        with open(fetched_vids, 'w') as file:
            json.dump(video_metadata, file, cls=DateTimeEncoder, indent=4)
        
        print(f"Data saved to {fetched_vids}")
        print(f"Number of videos fetched: {len(video_metadata)}")
        
        time.sleep(5)  # Pause to respect API limits
```

### Code to retrieve the video ids from the data in files filtering them by date, get their metadata with the `get_video_metadata()` function, and to save them in a new file.

```{python, eval=FALSE, python.reticulate = FALSE}
# assign directory
directory = 'data/willax_pbo_youtube_vids'

    
# Initialize an empty list to store all video IDs
all_videos_ids = []

# Iterate over files in the directory to collect video IDs
for filename in os.listdir(directory):
    filepath = os.path.join(directory, filename)  # Get the full path to the file
    
    # Check if file is a JSON file
    if os.path.isfile(filepath) and filepath.endswith('.json'):
        with open(filepath, 'r') as file:
            data = json.load(file)
            
            # Date range
            start_date = datetime(2020, 10, 1).timestamp()
            end_date = datetime(2021, 12, 31).timestamp()
            
            # Filter videos within the date range and collect their video_ids
            for video in data:
                publish_date = video['publish_date']
                if start_date <= publish_date <= end_date:
                    all_videos_ids.append(video['video_id'])
#print(f"Total videos to process: {len(all_videos_ids)}")

# Process each video ID
# Initialize matched_metadata outside the loop
matched_metadata = []

for video_id in all_videos_ids:
    print(f"Retrieving metadata for video_ID: {video_id}")
    vid_metadata = yt.get_video_metadata(video_id=video_id, 
                                         parser=P.parse_video_metadata, 
                                         part=['statistics', 'snippet'])
    matched_metadata.append(vid_metadata)
    time.sleep(5)

# Choose an appropriate filename for saving the data
video_dets_file = "all_videos_filtered.json"
vid_file_path = os.path.join(directory, video_dets_file)

with open(vid_file_path, 'w') as file:
    json.dump(matched_metadata, file, cls=DateTimeEncoder, indent=4)
# print(f"Data saved to {vid_file_path}")
# print(f"Number of videos saved: {len(matched_metadata)}")
```

### Compile videos filtered by term and save them in new file.

```{python, eval=FALSE, python.reticulate = FALSE}
filename = "all_videos_filtered.json"
filepath = os.path.join(directory, filename)  # Get the full path to the file

# Define the search terms
terms = ['Sinopharm', 'Sinovac', 'vacuna china', 'eficacia', 'vacuna', 'bustamante']
# Compile a regex pattern for case-insensitive search of these terms
pattern = re.compile(r'\b(?:' + '|'.join(re.escape(term) for term in terms) + r')\b', re.IGNORECASE)

matching_videos_count = 0

filtrd_lst = []

with open(filepath, 'r') as file:
    videos = json.load(file)
    #first_element = videos[0]
    #print(type(first_element), first_element)
    for video in videos:
        # Search in both the video title and description
        try:
            if type(video) == dict and (pattern.search(video['video_title']) or pattern.search(video['video_description'])):
                matching_videos_count += 1
                #print("Appending ", video['video_title'], "to filtrd_lst")
                filtrd_lst.append(video)

        except Exception as e:
            print(video)
print(f"Number of matching videos: {matching_videos_count}")
print(len(filtrd_lst))

extracted_videos = "sinoph_pbo_willax_disinfo.json"
vid_file_path = os.path.join(directory, extracted_videos)

with open(vid_file_path, 'w') as file:
    json.dump(filtrd_lst, file, cls=DateTimeEncoder, indent=4)

print(f"Data saved to {vid_file_path}")
print(f"Number of videos saved: {len(filtrd_lst)}")
```

## R code to preprocess and turn into a dataframe a json file's data

### Libraries to load
```{r, include=FALSE}
library(jsonlite)
library(tidyverse)
library(pacman)
library(lubridate)
library(ggrepel)
library(scales)
library(stringr)
library(tidyr)
library(gt)
```

```{r, include=FALSE}

file_path <- rio::import("data/willax_pbo_youtube_vids/sinoph_pbo_willax_disinfo.json")

# Create df. Data preprocess and cleaning
video_data <- file_path |>
  mutate(publish_date = as_date(as_datetime(video_publish_date, origin = "1970-01-01"))) |>
  # Replace 'FALSE' or any non-numeric values with 0
  mutate(video_dislike_count = ifelse(!is.na(as.numeric(video_dislike_count)), as.numeric(video_dislike_count), 0)) |>
  mutate(video_view_count = as.numeric(video_view_count)) |>
  mutate(video_comment_count = as.numeric(video_comment_count))  |>
  mutate(video_like_count = as.numeric(video_like_count)) |>
  mutate(Program = if_else(
    str_starts(video_title, "Ernesto"), 
    NA_character_,  # If title starts with Ernesto, set to NA or keep the original
    str_extract(video_title, "^[^:\\-]+(?=\\:|\\s?-\\s?)")  # Otherwise, extract the program name
  )) |>
  mutate(Program = str_replace_all(Program, 
                                   c("MilagrosLeivaEntrevista" = "Milagros Leiva Entrevista", 
                                     "Milagros LeivaEntrevista" = "Milagros Leiva Entrevista"))) |>
  mutate(video_title = case_when(
    str_detect(video_title, "^(Milagros)") ~ str_replace(video_title, ".*? - ", ""),
    TRUE ~ video_title)) |>
  mutate(video_title = case_when(
    str_detect(video_title, "^(Combutters: )") ~ str_replace(video_title, "Combutters: ", ""),
    TRUE ~ video_title
  )) |>
  mutate(video_title = case_when(
    str_detect(video_title, "^(Beto a Saber|Combutters)") ~ str_replace(video_title, ".*? - ", ""), TRUE ~ video_title
  )) |>
  select(channel = channel_title, 
         publish_date,
         Program,
         title = video_title,
         #description= video_description,
         views = video_view_count,
         comments = video_comment_count,
         likes = video_like_count,
         dislikes = video_dislike_count) |>
  arrange(publish_date)

video_data[[3]][1] = "Milagros Leiva Entrevista"
video_data[[3]][[3]] = "Beto a Saber"
video_data[[3]][70] = "PBO"
video_data[[3]][101] = "PBO"

# str(video_data)
# head(video_data)
# summary(video_data)

# Create a .csv file with the dataframe
#write.csv(video_data, "data/willax_pbo_youtube_vids/df_sinoph_pbo_willax_disinfo.csv", row.names = FALSE)
```

### Data frame and table organizing the videos by views 
```{r}
views_df <- video_data |>
  mutate(channel = str_replace_all(channel,
                                   "Willax Televisión", "Willax")) |>
  select(-dislikes) |>
  arrange(desc(views))

views_df_sliced <- views_df |>
  slice(1:12)

views_table_top <- views_df_sliced |>
  mutate(publish_date = as.factor(publish_date)) |>
  gt() |>
  tab_header(
    title = "Total of views per video (descending)"
  )|>
  data_color(columns = c(publish_date, views),
             direction = "column",
             target_columns = NULL,
             method = "auto",
             bins = 4,
             quantiles = 4,
             ordered = FALSE,
             reverse = TRUE)
views_table_top

views_table_tail <- tail(views_df, n=12) |>
  mutate(publish_date = as.factor(publish_date)) |>
  gt() |>
  tab_header(
    title = "Total of views per video (tail)"
  )|>
  data_color(columns = c(publish_date, views),
             direction = "column",
             target_columns = NULL,
             method = "auto",
             bins = 4,
             quantiles = 4,
             ordered = FALSE,
             reverse = FALSE)
views_table_tail
```

### Data frame and table organizing the videos by date, focusing on March
```{r}
march_2021_videos <- video_data |>
  filter(publish_date >= as_date("2021-03-01") & publish_date <= as_date("2021-03-31")) |> 
  mutate(channel = str_replace_all(channel,
                                   "Willax Televisión", "Willax")) |>
  select(-dislikes)

march_2021_table <- march_2021_videos |>
  mutate(publish_date = as.character(publish_date)) |>
  gt() |>
  tab_header(
    title = "Data March 2021 Videos"
  )|>
  data_color(columns = publish_date,
             rows = starts_with("2021-03-06"),
             direction = "row",
             target_columns = NULL,
             method = "auto",
             palette = "yellow")
march_2021_table
```

### Data frame and table with the statistics
```{r}
stat_summ_df <- summary(video_data) |>
  as.data.frame.matrix() |>
  as_tibble()|>
  select(-c("  channel", "  Program"))

colnames(stat_summ_df) <- gsub("^\\s+", "", colnames(stat_summ_df))

stat_summ_df <- stat_summ_df |>
  rename(videos = title)

gt_table <- stat_summ_df |>
  gt() |>
  tab_header(
    title = "Video Statistics Summary"
  ) |>
  cols_label(
    publish_date = "Publish Date",
    videos = "Videos",
    views = "Views",
    comments = "Comments",
    likes = "Likes",
    dislikes = "Dislikes"
  ) |>
  cols_move_to_start(
    columns = c(videos)
  )
gt_table
```



### Case Study
```{r, eval=FALSE, include=TRUE}
file <- rio::import("data/willax_pbo_youtube_vids/wil_pbo_sinoph_comments.json")
View(file)

View(file[[1]])
colnames(file[[1]])

video_comments <- (file[[1]]) |>
 mutate(comment_publish_date = as_datetime(comment_publish_date, origin = "1970-01-01")) |>
 mutate(comment_like_count = ifelse(!is.na(as.numeric(comment_like_count)), as.numeric(comment_like_count), 0)) |>
 mutate(reply_count = as.numeric(reply_count)) |>
 mutate(attitude = NA_character_) |>
 mutate(comment_date = as_date(comment_publish_date),
        comment_time = format(comment_publish_date, "%H:%M:%S")) |>
 select(video_id,
        comment_date,
        comment_time,
        comment_id,
        comment = text,
        attitude,
        comment_likes = comment_like_count,
        replies = reply_count,
        commenter_channel_id,
        commenter_channel_display_name,
        commenter_rating,
        comment_parent_id) |>
 arrange(comment_date)

# str(video_comments)
# head(video_comments)
summary(video_comments)

#write.csv(video_comments, "data/willax_pbo_youtube_vids/df_sinoph_comment_dis_code1.csv", row.names = FALSE)
```

```{r, echo=FALSE}
coded_file <- rio::import("data/willax_pbo_youtube_vids/df_sinoph_comment_dis_code1JC.csv")

video_comments_summary <- coded_file |>
  mutate(replies = as.numeric(replies)) |>
  mutate(attitude = as.numeric(attitude)) |>
  filter(!is.na(attitude)) |>
  mutate(attitude = case_when(
    attitude == 0 ~ "Neutral",
    attitude == 1 ~ "Favor (Politically)",
    attitude == 2 ~ "Favor (Sinopharm/Genocide)",
    attitude == 3 ~ "Against",
    TRUE ~ as.character(attitude)  # Safety catch to convert any other numbers to character
  )) |> 
  group_by(attitude) |>
  summarize(
    total_commenters = n(),
    unique_commenters = n_distinct(commenter_channel_display_name, na.rm = TRUE),
    non_unique_commenters = total_commenters - unique_commenters,
    total_likes = sum(comment_likes, na.rm = TRUE),
    .groups = 'drop'
  ) |>
  arrange(attitude)

# Dataframe summarising the coded obsercations
attude_summ_df <-coded_file |>
  summarise(
    Coded_Observations = sum(!is.na(attitude)),
    Total_NAs = sum(is.na(attitude))
  )
# print(summary_df) #Check and debug

# Table for attitude summary
attude_summ_tab <- attude_summ_df |>
  gt() |>
  tab_header(
    title = "Attitude Totals"
  ) |>
  cols_label(
    Coded_Observations = "Coded Observations",
    Total_NAs = "NAs"
  )
attude_summ_tab

# Video Comments Table --------
comm_table <- video_comments_summary |>
  gt() |>
  tab_header(
    title = "Comments Summary"
  ) |>
  cols_label(
    attitude = "Attitude Toward Video",
    total_commenters = "Total Commenters",
    unique_commenters = "Unique Commenters",
    non_unique_commenters = "Recurrent Commenters",
    total_likes = "Total Likes",
  ) |>
  data_color(
    columns = c(unique_commenters, non_unique_commenters),
    direction = "row",
    method = "auto")
comm_table

# Unique and Recurrent Commenters ------
# Calculate non-unique commenters and their posting frequency
commenter_frequencies <- coded_file |>
  count(commenter_channel_display_name, sort = TRUE, name = "posts_count") |>
  filter(commenter_channel_display_name != "NA") |>
  count(posts_count, name = "commenter_count") 

comm_freq_tbl <- commenter_frequencies |>
  gt() |>
  tab_header(
    title = "Commenter Frequency"
  ) |>
  cols_label(
    commenter_count = "Commenters",
    posts_count = "Posts"
  ) |>
  data_color(columns = posts_count,
             direction = "column",
             target_columns = NULL,
             method = "auto",
             bins = 8,
             quantiles = 4,
             ordered = FALSE,
             reverse = TRUE)|>
  cols_move_to_start(
    columns = c(commenter_count)
  )
comm_freq_tbl
```

```{r, eval=FALSE}
library(jsonlite)
library(tidyverse)
library(pacman)
library(lubridate)
library(scales)

file_path <- rio::import("data/willax_pbo_youtube_vids/sinoph_pbo_willax_disinfo.json")

video_data <- file_path |>
  mutate(publish_date = as_date(as_datetime(video_publish_date, origin = "1970-01-01"))) |>
  # Replace 'FALSE' or any non-numeric values with 0
  mutate(video_dislike_count = ifelse(!is.na(as.numeric(video_dislike_count)), as.numeric(video_dislike_count), 0)) |>
  mutate(video_view_count = as.numeric(video_view_count)) |>
  mutate(video_comment_count = as.numeric(video_comment_count))  |>
  mutate(video_like_count = as.numeric(video_like_count)) |>
  mutate(Program = str_extract(video_title, "^(.*?)(?=\\:|\\s-|\\s\\-|\\-)")) |>
  mutate(Program = str_replace_all(Program, 
                                   c("MilagrosLeivaEntrevista" = "Milagros Leiva Entrevista", 
                                     "Milagros LeivaEntrevista" = "Milagros Leiva Entrevista"))) |>
  select(channel = channel_title, 
         publish_date,
         Program,
         title = video_title,
         #description= video_description,
         views = video_view_count,
         comments = video_comment_count,
         likes = video_like_count,
         dislikes = video_dislike_count) |>
  arrange(publish_date)

video_data[[3]][1] = "Combutters"
video_data[[3]][70] = "Combutters"
video_data[[3]][101] = "Combutters"

str(video_data)
head(video_data)
summary(video_data)
```

```{r, echo=TRUE}
# Plotting number of views by date and program
video_data |> ggplot(aes(x = publish_date, y = views, color = Program, group = Program)) + 
  geom_jitter() +
  labs(title = "Video View Count", x = "Date", y = "Views") +
  theme_minimal(base_size = 10) +
  theme(
    title = element_text(size=15),
    axis.title = element_text(color = "darkgrey"),
    legend.position = "bottom",  
    legend.title = element_blank(),  # Remove legend title
    axis.text.x = element_text(size = 7),
    panel.grid.major.x = element_blank(),  # Remove vertical grid lines
    #panel.grid.minor.x = element_blank(),  # Remove minor vertical grid lines
    aspect.ratio = 1/3  # Set aspect ratio for wider plot
  ) +
  scale_x_date(date_labels = "%b %y", date_breaks = "1 month") +
  scale_y_continuous(labels = label_comma())  

# Plotting number of comments by date and program
video_data |> ggplot(aes(x = publish_date, y = comments, color = Program, group = Program)) + 
  geom_jitter() +
  labs(title = "Video Comment Count", x = "Date", y = "Comments") +
  theme_minimal(base_size = 10) +
  theme(
    title = element_text(size=15),
    axis.title = element_text(color = "darkgrey"),
    legend.position = "bottom",  
    legend.title = element_blank(),  # Remove legend title
    axis.text.x = element_text(size = 7),
    panel.grid.major.x = element_blank(),  # Remove vertical grid lines
    #panel.grid.minor.x = element_blank(),  # Remove minor vertical grid lines
    aspect.ratio = 1/3  # Set aspect ratio for wider plot
  ) +
  scale_x_date(date_labels = "%b %y", date_breaks = "1 month") +
  scale_y_continuous(labels = label_comma())  

# Plotting number of likes by date and program
video_data |> ggplot(aes(x = publish_date, y = likes, color = Program, group = Program)) + 
  geom_jitter() +
  labs(title = "Video Like Count", x = "Date", y = "Likes") +
  theme_minimal(base_size = 10) +
  theme(
    title = element_text(size=15),
    axis.title = element_text(color = "darkgrey"),
    legend.position = "bottom",  
    legend.title = element_blank(),  # Remove legend title
    axis.text.x = element_text(size = 7),
    panel.grid.major.x = element_blank(),  # Remove vertical grid lines
    #panel.grid.minor.x = element_blank(),  # Remove minor vertical grid lines
    aspect.ratio = 1/3  # Set aspect ratio for wider plot
  ) +
  scale_x_date(date_labels = "%b %y", date_breaks = "1 month") +
  scale_y_continuous(labels = label_comma()) 
```

## Google Trends API

### Python code to access Pytrends, a non-official Google Trends API.

```{python, eval=FALSE, python.reticulate = FALSE}
#!pip install pytrends
#!pip install tables

import pandas as pd
import os
import re
import json
from pytrends.request import TrendReq
pytrend = TrendReq()

directory = 'data/google_trends'

# Build payloads (Google Trends searches via the pytrends API)
#keywords: lists with terms to search
keywords1 = ['vacuna Sinopharm', 'sinopharm', 'sinopharm vacuna', 'vacuna covid']
pytrend.build_payload(kw_list=keywords1, timeframe='2020-10-01 2022-06-30', geo='PE')
interest_over_time_df1 = pytrend.interest_over_time()
#print(interest_over_time_df1)

keywords2 = ['sinopharm efectividad', 'vacuna sinopharm efectividad', 'sinopharm vacuna efectividad', 'efictividad sinopharm', 'vacuna covid']
pytrend.build_payload(kw_list=keywords2, timeframe='2020-10-01 2022-06-30', geo='PE')
interest_over_time_df2 = pytrend.interest_over_time()
#print(interest_over_time_df2)

keywords3 = ['vacuna pfizer', 'pfizer', 'pfizer vacuna', 'vacuna covid' ]
pytrend.build_payload(kw_list=keywords3, timeframe='2020-10-01 2022-06-30', geo='PE')
interest_over_time_df3 = pytrend.interest_over_time()
#print(interest_over_time_df3)

keywords4 = ['pfizer efectividad', 'efectividad pfizer', 'vacuna pfizer efectividad', 'pfizer efectividad vacuna', 'vacuna covid']
pytrend.build_payload(kw_list=keywords4, timeframe='2020-10-01 2022-06-30', geo='PE')
interest_over_time_df4 = pytrend.interest_over_time()
#print(interest_over_time_df4)

keywords5 = ["covid vacuna", "AstraZeneca","AstraZeneca efectividad", 'vacuna covid']
pytrend.build_payload(kw_list=keywords5, timeframe='2020-10-01 2022-06-30', geo='PE')
interest_over_time_df5 = pytrend.interest_over_time()
#print(interest_over_time_df5)

# Save pytrend dataframes into csv files
'''filename1 = "interest_over_time_df1.csv"
filepath1 = os.path.join(directory, filename1)  # Get the full path to the file
interest_over_time_df1.to_csv(filepath1)

filename2 = "interest_over_time_df2.csv"
filepath2 = os.path.join(directory, filename2)  # Get the full path to the file
interest_over_time_df2.to_csv(filepath2)

filename3 = "interest_over_time_df3.csv"
filepath3 = os.path.join(directory, filename3)  # Get the full path to the file
interest_over_time_df3.to_csv(filepath3)

filename4 = "interest_over_time_df4.csv"
filepath4 = os.path.join(directory, filename4)  # Get the full path to the file
interest_over_time_df4.to_csv(filepath4)

filename5 = "interest_over_time_df5.csv"
filepath5 = os.path.join(directory, filename5)  # Get the full path to the file
interest_over_time_df5.to_csv(filepath5)'''
```

### R code put together, preprocess, and plot data from Google Trends

```{r}
library(tidyverse)
library(tidyr)
library(ggrepel)

df1 <- rio::import("data/google_trends/interest_over_time_df1.csv") |>
  select(-isPartial) |>
  mutate(date = as.Date(date, format = "%Y-%m-%d")) |>
  pivot_longer(cols = -date, names_to = "search term", values_to = "interest")

df2 <- rio::import("data/google_trends/interest_over_time_df2.csv") |>
  select(-isPartial) |>
  mutate(date = as.Date(date, format = "%Y-%m-%d"))|>
  pivot_longer(cols = -date, names_to = "search term", values_to = "interest")

df3 <- rio::import("data/google_trends/interest_over_time_df3.csv") |>
  select(-isPartial) |>
  mutate(date = as.Date(date, format = "%Y-%m-%d")) |>
  pivot_longer(cols = -date, names_to = "search term", values_to = "interest")

df4 <- rio::import("data/google_trends/interest_over_time_df4.csv") |>
  select(-isPartial) |>
  mutate(date = as.Date(date, format = "%Y-%m-%d")) |>
  pivot_longer(cols = -date, names_to = "search term", values_to = "interest")

df5 <- rio::import("data/google_trends/interest_over_time_df5.csv") |>
  select(-isPartial) |>
  mutate(date = as.Date(date, format = "%Y-%m-%d")) |>
  pivot_longer(cols = -date, names_to = "search term", values_to = "interest")

df_gen_trnds_pe <- rio::import("data/google_trends/trends_peru_2020-2022_multiTimeline.csv") |>
  rename(date = Week) |>
  mutate(date = as.Date(date, format = "%Y-%m-%d")) |>
  mutate(across(-date, as.character)) |>
  mutate(across(-date, ~str_replace(., "<1", "0.5"))) |>
  mutate(across(-date, as.numeric)) |>
  pivot_longer(cols = -date, names_to = "search term", values_to = "interest")

```

### Google Trends Plots

```{r, echo=FALSE}
# Plotting
df_gen_trnds_pe |> ggplot(aes(x = date, y = interest, color = `search term`, group = `search term`)) +
  geom_line() +
  labs(title = "Interest Over Time", x = "Date", y = "Search Interest") +
  theme_minimal(base_size = 14) +
  theme(
    legend.position = "bottom",
    legend.title = element_blank(),  # Remove legend title
    panel.grid.major.x = element_blank(),  # Remove vertical grid lines
    #panel.grid.minor.x = element_blank(),  # Remove minor vertical grid lines
    aspect.ratio = 1/3  # aspect ratio for wider plot
  ) +
  scale_x_date(date_labels = "%b %d", date_breaks = "2 month") 


# Plotting
df_gen_trnds_pe |> ggplot(aes(x = date, y = interest, color = `search term`, group = `search term`)) +
  geom_line() +
  labs(title = "Interest Over Time", x = "Date", y = "Search Interest") +
  theme_minimal(base_size = 10) +
  theme(
    legend.position = "bottom",
    legend.title = element_blank(),  # Remove legend title
    axis.text.x = element_text(size = 7),
    panel.grid.major.x = element_blank(),  # Remove vertical grid lines
    #panel.grid.minor.x = element_blank(),  # Remove minor vertical grid lines
    aspect.ratio = 1/3  # aspect ratio for wider plot
  ) +
  scale_x_date(date_labels = "%b %y", date_breaks = "1 month") 

df1 |> 
  ggplot(aes(x = date, y = interest, color = `search term`, group = `search term`)) +
  geom_line() +
  labs(title = "Interest Over Time", x = "Date", y = "Search Interest") +
  theme_minimal(base_size = 10) +
  theme(
    legend.position = "bottom",  
    legend.title = element_blank(),
    axis.text.x = element_text(size = 7),
    panel.grid.major.x = element_blank(), 
    #panel.grid.minor.x = element_blank(),
    aspect.ratio = 1/3  
  ) +
  scale_x_date(date_labels = "%b %y", date_breaks = "1 month")  

df2 |> ggplot(aes(x = date, y = interest, color = `search term`, group = `search term`)) +
  geom_line() +
  labs(title = "Interest Over Time", x = "Date", y = "Search Interest") +
  theme_minimal(base_size = 10) +
  theme(
    legend.position = "bottom",  
    legend.title = element_blank(),
    axis.text.x = element_text(size = 7),
    panel.grid.major.x = element_blank(),
    #panel.grid.minor.x = element_blank(),
    aspect.ratio = 1/3  
  ) +
  scale_x_date(date_labels = "%b %y", date_breaks = "1 month") 

df3 |> ggplot(aes(x = date, y = interest, color = `search term`, group = `search term`)) +
  geom_line() +
  labs(title = "Interest Over Time", x = "Date", y = "Search Interest") +
  theme_minimal(base_size = 10) +
  theme(
    legend.position = "bottom", 
    legend.title = element_blank(),  
    axis.text.x = element_text(size = 7),
    panel.grid.major.x = element_blank(),  
    #panel.grid.minor.x = element_blank(),  
    aspect.ratio = 1/3  
  ) +
  scale_x_date(date_labels = "%b %y", date_breaks = "1 month") 

df4 |> ggplot(aes(x = date, y = interest, color = `search term`, group = `search term`)) +
  geom_line() +
  labs(title = "Interest Over Time", x = "Date", y = "Search Interest") +
  theme_minimal(base_size = 10) +
  theme(
    legend.position = "bottom",  
    legend.title = element_blank(),  
    axis.text.x = element_text(size = 7),
    panel.grid.major.x = element_blank(),
    #panel.grid.minor.x = element_blank(),
    aspect.ratio = 1/3  
  ) +
  scale_x_date(date_labels = "%b %y", date_breaks = "1 month") 

df5 |> ggplot(aes(x = date, y = interest, color = `search term`, group = `search term`)) +
  geom_line() +
  labs(title = "Interest Over Time", x = "Date", y = "Search Interest") +
  theme_minimal(base_size = 10) +
  theme(
    legend.position = "bottom",  
    legend.title = element_blank(),  
    axis.text.x = element_text(size = 7),
    panel.grid.major.x = element_blank(),  
    #panel.grid.minor.x = element_blank(),  
    aspect.ratio = 1/3  
  ) +
  scale_x_date(date_labels = "%b %y", date_breaks = "1 month") 
```


## Python code to get comments to videos beteen March and September 2021 using the `get_video_comments()` function and to create a json file to store them

```{python, eval=FALSE, python.reticulate = FALSE}
directory = 'data/willax_pbo_youtube_vids'

video_file = "sinoph_pbo_willax_disinfo.json"
vid_file_path = os.path.join(directory, video_file)

with open(vid_file_path, 'r') as file:
    print(f"Opening {vid_file_path}")
    videos_data = json.load(file)

df_willax_sinoph = pd.DataFrame(videos_data)
df_willax_sinoph.tail(5)

df_willax_sinoph.info

# Convert UNIX timestamp to human-readable date (the unit='s' specifies that the timestamp is in seconds)
df_willax_sinoph["video_publish_date"] = pd.to_datetime(df_willax_sinoph["video_publish_date"], unit='s', origin='unix')
df_willax_sinoph["video_publish_date"] = df_willax_sinoph["video_publish_date"].dt.date
df_willax_sinoph = df_willax_sinoph.sort_values(by="video_publish_date", ascending=True)
#print(df_willax_sinoph)

willax_sinoph_sub = df_willax_sinoph[['video_publish_date', 'video_id', 'video_title', 
                                      'video_description', 'video_view_count', 'video_comment_count', 
                                      'channel_title', 'channel_id']].rename(columns = {
        'video_publish_date': 'publish_date',
        'video_title': 'title', 
        'video_description': 'description',
        'video_view_count': 'views', 
        'video_comment_count': 'comment_count',
        'channel_title': 'channel' 
    }
)
willax_sinoph_sub

wil_sino_mar_sep_21 = willax_sinoph_sub[
    (willax_sinoph_sub["publish_date"] >= datetime.strptime('2021-03-01', '%Y-%m-%d').date()) &
    (willax_sinoph_sub["publish_date"] <= datetime.strptime('2021-09-30', '%Y-%m-%d').date())
]

wil_sino_mar_sep_21.shape

video_list = ['9K9Vpk2N38M', 'FmHqf_7sgnE', 'FmHqf_7sgnE', 'mLhonH1bDOA', 'N8TCILfoxqk', '61MuoV0b_wc', 'P_CqzxTz8qc', 
 'L7zdcppc2Ro', 'PagOb0eOyOQ', 'YBuaoRv3iIg', 'Uq0sf0pvo60', 'Yiaxey-NaVQ', 'zDFmWHAVPdE', 'g1rOO2hH9Nw', 
 'SrybIYyjXS0', 'nFMhTNYVmyU',  '98_C_oIgrok', 'jOs8UNJKsFU', '5biabFQuzvo', '2OQtHMsi6G4', 'ER8T0SRG-vQ']

# Dictionary to store the comments for each video
video_comments_dict = {}

for video_id in video_list:
    try:
        vid_comments = yt.get_video_comments(video_id=video_id, parser=P.parse_comment_metadata, get_replies=True,
                                             max_results=None, next_page_token=False,
                                             part=['snippet'])
        # Assuming yt.get_video_comments raises an exception on failure and does not return a response object
        # Store the comments in a dictionary using the video_id as the key
        video_comments_dict[video_id] = vid_comments
    except Exception as e:
        # Handle potential exceptions, such as a network error or API error
        print("An error occurred for video ID {}: {}".format(video_id, e))
    time.sleep(20)  # Delay of 20 seconds between requests

#video_comments_dict

comments_file = "wil_pbo_sinoph_comments.json"
file_path = os.path.join(directory, comments_file)

with open(file_path, 'w') as file:
    json.dump(video_comments_dict, file, cls=DateTimeEncoder, indent=4)

print(f"Data saved to {file_path}")
```

## Pysentimiento for sentiment, emotion, and hate-speech analysis of Willax Videos

```{python, eval=FALSE, python.reticulate = FALSE}
import pandas as pd
import os
import re
import json
import time # to call the .sleep() method to include a pause that respects potential respect API limits
import matplotlib.pyplot as plt
import transformers

from datetime import datetime
from pysentimiento import create_analyzer
from pysentimiento.preprocessing import preprocess_tweet

# ------------------------------------------------------------------------------------
from datetime import datetime
class DateTimeEncoder(json.JSONEncoder):
    """Custom encoder for datetime objects."""
    def default(self, obj):
        if isinstance(obj, datetime):
            return obj.isoformat()  # Convert datetime to ISO format string
        # Let the base class default method raise the TypeError
        return super().default(obj)
# ------------------------------------------------------------------------------------

transformers.logging.set_verbosity(transformers.logging.ERROR)
sentiment_analyzer = create_analyzer(task="sentiment", lang="es")
hate_speech_analyzer = create_analyzer(task="hate_speech", lang="es")
emotion_analyzer = create_analyzer(task="emotion", lang="es")

# ------------------------------------------------------------------------------------
filepath = "data/willax_pbo_youtube_vids/wil_pbo_sinoph_comments.json"
with open(filepath, 'r') as file:
    video_comments = json.load(file)

def analyze_comment(comment_text):
    from pysentimiento import create_analyzer
    from pysentimiento.preprocessing import preprocess_tweet
    
   
    processed_text = preprocess_tweet(comment_text)
    sentiment_result = sentiment_analyzer.predict(processed_text)
    hate_speech_result = hate_speech_analyzer.predict(processed_text)
    emotion_result = emotion_analyzer.predict(processed_text)

    # Return a dictionary containing the results
    return {
        'sentiment': sentiment_result,
        'hate_speech': hate_speech_result,
        'emotion': emotion_result
    }
    
# ------------------------------------------------------------------------------------
# Dictionary to store analyzed comments by video ID
anlzd_vcoms_dict = {}

# Iterate through each video ID and its corresponding comments list
for video_id, comments in video_comments.items():
    # List to hold the analyzed comments for the current video
    analyzed_comments = []
    
    # Iterate through each comment dictionary in the comments list
    for comment in comments:
        # Apply your sentiment and emotion analysis on the comment text
        analysis_result = analyze_comment(comment['text'])
        # Add the analysis result to the list of analyzed comments for this video
        analyzed_comments.append(analysis_result)

    # Store the analyzed comments in the dictionary with the video_id as key
    anlzd_vcoms_dict[video_id] = analyzed_comments

# 'anlzd_vcoms_dict' contains analyzed data ready for further use.
# ------------------------------------------------------------------------------------
type(anlzd_vcoms_dict)
len(anlzd_vcoms_dict)
anlzd_vcoms_dict.keys()
anlzd_vcoms_dict.values()


# dict_values([[{'sentiment': AnalyzerOutput(output=NEU, probas={NEU: 0.855, POS: 0.088, NEG: 0.058}), 'hate_speech': AnalyzerOutput(output=[], probas={hateful: 0.072, targeted: 0.016, aggressive: 0.028}), 'emotion': AnalyzerOutput(output=others, probas={others: 0.988, joy: 0.006, surprise: 0.002, anger: 0.001, sadness: 0.001, disgust: 0.001, fear: 0.001})}, {'sentiment': AnalyzerOutput(output=NEG, probas={NEG: 0.959, NEU: 0.031, POS: 0.009}), 'hate_speech': AnalyzerOutput(output=[], probas={hateful: 0.106, targeted: 0.020, aggressive: 0.086}), 'emotion': AnalyzerOutput(output=anger, probas={anger: 0.749, disgust: 0.243, sadness: 0.002, others: 0.002, fear: 0.002, surprise: 0.001, joy: 0.001})}, {'sentiment': AnalyzerOutput(output=POS, probas={POS: 0.958, NEU: 0.038, NEG: 0.004}), 'hate_speech': AnalyzerOutput(output=[], probas={hateful: 0.011, targeted: 0.011, aggressive: 0.009}), 'emotion': AnalyzerOutput(output=joy, probas={joy: 0.517, others: 0.433, surprise: 0.044, disgust: 0.003, anger: 0.002, fear: 0.002, sadness: 0.001})}, {'sentiment': AnalyzerOutput(output=NEU, probas={NEU: 0.671, POS: 0.171, NEG: 0.158}), 'hate_speech': AnalyzerOutput(output=[], probas={hateful: 0.023, targeted: 0.015, aggressive: 0.013}), 'emotion': AnalyzerOutput(output=others, probas={others: 0.952, joy: 0.031, sadness: 0.008, anger: 0.003, surprise: 0.003, disgust: 0.001, fear: 0.001})}, {'sentiment': AnalyzerOutput(output=NEG, probas={NEG: 0.598, NEU: 0.302, POS: 0.101}), 'hate_speech': AnalyzerOutput(output=['hateful', 'aggressive'], probas={hateful: 0.911, targeted: 0.348, aggressive: 0.834}), 'emotion': AnalyzerOutput(output=anger, probas={anger: 0.827, disgust: 0.160, others: 0.008, sadness: 0.003, joy: 0.001, fear: 0.001, surprise: 0.001})}, {'sentiment': AnalyzerOutput(output=NEG, probas={NEG: 0.545, NEU: 0.419, POS: 0.036}), 'hate_speech': AnalyzerOutput(output=[], probas={hateful: 0.247, targeted: 0.014, aggressive: 0.059}), 'emotion': AnalyzerOutput(output=others, probas={others: 0.964, anger: 0.021, sadness: 0.004, disgust: 0.004, surprise: 0.002, fear: 0.002, joy: 0.001})}, {'sentiment': AnalyzerOutput(output=NEG, probas={NEG: 0.970, NEU: 0.025, POS: 0.005}), 'hate_speech': AnalyzerOutput(output=[], probas={hateful: 0.073, targeted: 0.007, aggressive: 0.060}), 'emotion': AnalyzerOutput(output=anger, probas={anger: 0.741, disgust: 0.248, sadness: 0.006, others: 0.002, fear: 0.002, joy: 0.001, surprise: 0.001})}, {'sentiment': AnalyzerOutput(output=NEU, probas={NEU: 0.756, NEG: 0.218, POS: 0.025}), 'hate_speech': AnalyzerOutput(output=[], probas={hateful: 0.204, targeted: 0.012, aggressive: 0.057}), 'emotion': AnalyzerOutput(output=others, probas={others: 0.973, surprise: 0.008, fear: 0.007, sadness: 0.007, anger: 0.003, disgust: 0.002, joy: 0.001})}, {'sentiment': AnalyzerOutput(output=NEG, probas={NEG: 0.605, NEU: 0.358, POS: 0.036}), 'hate_speech': AnalyzerOutput(output=['hateful'], probas={hateful: 0.546, targeted: 0.013, aggressive: 0.130}), 'emotion': AnalyzerOutput(output=others, probas={others: 0.717, anger: 0.148, sadness: 0.074, disgust: 0.036, joy: 0.017, surprise: 0.004, fear: 0.004})}, {'sentiment': AnalyzerOutput(output=NEU, probas={NEU: 0.694, NEG: 0.253, POS: 0.054}), 'hate_speech': AnalyzerOutput(output=['hateful'], probas={hateful: 0.591, targeted: 0.010, aggressive: 0.146}), 'emotion': AnalyzerOutput(output=others, probas={others: 0.954, anger: 0.011, surprise: 0.010, fear: 0.009, disgust: 0.007, sadness: 0.005, joy: 0.004})}, {'sentiment': AnalyzerOutput(output=NEU, probas={NEU: 0.652, POS: 0.273, NEG: 0.075}), 'hate_speech': AnalyzerOutput(output=['hateful'], probas={hateful: 0.549, targeted: 0.015, aggressive: 0.134}), 'emotion': AnalyzerOutput(output=others, probas={others: 0.968, joy: 0.020, surprise: 0.005, fear: 0.003, anger: 0.003, disgust: 0.001, sadness: 0.001})}, {'sentiment': AnalyzerOutput(output=NEG, probas={NEG: 0.945, NEU: 0.050, POS: 0.005}), 'hate_speech': AnalyzerOutput(output=['hateful'], probas={hateful: 0.839, targeted: 0.009, aggressive: 0.363}), 'emotion': AnalyzerOutput(output=anger, probas={anger: 0.908, disgust: 0.080, sadness: 0.007, others: 0.004, fear: 0.001, surprise: 0.000, joy: 0.000})}, {'sentiment': AnalyzerOutput(output=NEG, probas={NEG: 0.533, NEU: 0.437, POS: 0.030}), 'hate_speech': AnalyzerOutput(output=[], probas={hateful: 0.030, targeted: 0.018, aggressive: 0.017}), 'emotion': AnalyzerOutput(output=others, probas={others: 0.945, surprise: 0.039, joy: 0.007, anger: 0.004, fear: 0.002, sadness: 0.001, disgust: 0.001})}, {'sentiment': AnalyzerOutput(output=NEG, probas={NEG: 0.980, NEU: 0.015, POS: 0.006}), 'hate_speech': AnalyzerOutput(output=[], probas={hateful: 0.048, targeted: 0.004, aggressive: 0.025}), 'emotion': AnalyzerOutput(output=disgust, probas={disgust: 0.482, anger: 0.478, sadness: 0.023, others: 0.011, fear: 0.003, joy: 0.001, surprise: 0.001})}, {'sentiment': AnalyzerOutput(output=NEG, probas={NEG: 0.954, NEU: 0.039, POS: 0.006}), 'hate_speech': AnalyzerOutput(output=['hateful', 'aggressive'], probas={hateful: 0.817, targeted: 0.016, aggressive: 0.522}), 'emotion': AnalyzerOutput(output=anger, probas={anger: 0.880, disgust: 0.115, sadness: 0.002, fear: 0.001, others: 0.001, joy: 0.001, surprise: 0.000})}, {'sentiment': AnalyzerOutput(output=NEG, probas={NEG: 0.975, NEU: 0.022, POS: 0.003}), 'hate_speech': AnalyzerOutput(output=['hateful', 'aggressive'], probas={hateful: 0.794, targeted: 0.018, aggressive: 0.548}), 'emotion': AnalyzerOutput(output=anger, probas={anger: 0.901, disgust: 0.091, sadness: 0.005, others: 0.001, fear: 0.001, joy: 0.001, surprise: 0.000})},

# -------------------------------------------------------------------------------
anlzd_vcoms_file = "data/willax_pbo_youtube_vids/wil_pbo_sinoph_anlzd_vcoms.json"

with open(anlzd_vcoms_file, 'w') as file:
    json.dump(anlzd_vcoms_dict, file, cls=DateTimeEncoder, indent=4)

print(f"Data saved to {anlzd_vcoms_file}")

'TypeError: Object of type AnalyzerOutput is not JSON serializable'

```

```{python, eval=FALSE, python.reticulate = FALSE}

### EXAMPLE 1
preprocess_tweet(comment)

print(sentiment_analyzer.predict(comment))
print(hate_speech_analyzer.predict(comment))
print(emotion_analyzer.predict(comment))

# Output
'CAGASTI A LA CARCEL POR GENOCIDIO,  EL PODER JUDICIAL DEBE PRONUNCIARSE AL RESPECTO, LOS FISCALES DE OFICIO DEBEN PROCESARLO POR INEPTO, la vacuna Rusa era gratis, y no podia coimear,  por eso prefirio comprar los placebos Chinos,  porque ahi si cutrea de lo lindo,  tipo corrupto, es para arrastrarlo de las barbas  por palacio de gobierno hasta echarlo a las calles como el despojo humano que es.'

AnalyzerOutput(output=NEG, probas={NEG: 0.959, NEU: 0.031, POS: 0.009})
AnalyzerOutput(output=[], probas={hateful: 0.106, targeted: 0.020, aggressive: 0.086})
AnalyzerOutput(output=anger, probas={anger: 0.749, disgust: 0.243, sadness: 0.002, others: 0.002, fear: 0.002, surprise: 0.001, joy: 0.001})

### EXAMPLE 2
comment_2 = "SIN DUDA ES EL MEJOR NOTICIERO"
print(sentiment_analyzer.predict(comment_2))
print(hate_speech_analyzer.predict(comment_2))
print(emotion_analyzer.predict(comment_2))

# Output
AnalyzerOutput(output=POS, probas={POS: 0.958, NEU: 0.038, NEG: 0.004})
AnalyzerOutput(output=[], probas={hateful: 0.011, targeted: 0.011, aggressive: 0.009})
AnalyzerOutput(output=joy, probas={joy: 0.517, others: 0.433, surprise: 0.044, disgust: 0.003, anger: 0.002, fear: 0.002, sadness: 0.001})

### EXAMPLE 3
comment_3 = "SAGASTI Y EL MITOMANO DE VIZCARRA SON GENOSIDAS DEBEN SER FUSILADOS EN PUBLICO CUANTAS FAMILIAS LLORAN SUS MUERTOS POR QUE SE CIERRAN CON LAS VACUNAS CHINAS QUE NO SIRVE NI DESINFECTANTE."

print(sentiment_analyzer.predict(comment_3))
print(hate_speech_analyzer.predict(comment_3))
print(emotion_analyzer.predict(comment_3))

# Output
AnalyzerOutput(output=NEG, probas={NEG: 0.970, NEU: 0.025, POS: 0.005})
AnalyzerOutput(output=[], probas={hateful: 0.073, targeted: 0.007, aggressive: 0.060})
AnalyzerOutput(output=anger, probas={anger: 0.741, disgust: 0.248, sadness: 0.006, others: 0.002, fear: 0.002, joy: 0.001, surprise: 0.001})

```
